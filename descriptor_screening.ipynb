{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import warnings\n",
    "import keras.backend as K\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "conduct_path=\"E:/code\"\n",
    "filename = \"foam1.csv\"\n",
    "\n",
    "\n",
    "#define a function to load csv data set, by changing the directory name conduct_path and file name filename, the function can read different csv files\n",
    "def load_conduct_data():\n",
    "    csv_path = os.path.join(conduct_path, filename)\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "def r2(y_test, y_pred):\n",
    "    a = K.square(y_pred - y_test)\n",
    "    b = K.sum(a)\n",
    "    c = K.mean(y_test)\n",
    "    d = K.square(y_test - c)\n",
    "    e = K.sum(d)\n",
    "    f = 1 - b/e\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the file and use conduct to represent the loaded content\n",
    "conduct = load_conduct_data()\n",
    "\n",
    "#change the loaded content to numpy format\n",
    "c = np.array(conduct)\n",
    "\n",
    "#select the first 131 columns as features, i.e. [0,130]\n",
    "X = c[:,:131]\n",
    "\n",
    "#select the label, the label is the output, in this data set, 131, 132, 133 list the thermal conductivity of the foam in three directions, there is no special direction, so any direction of the feature is selected as the processing index\n",
    "y = c[:,131:132]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "#show the correlation coefficient between the features and the label\n",
    "corr = conduct.corr().abs()\n",
    "sns.heatmap(data=corr)\n",
    "plt.title(\"Coefficients\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "#Get the absolute value of the correlation coefficient of the thermal conductivity in the X direction\n",
    "cccc = conduct.corr().abs()['CX']\n",
    "value = []\n",
    "\n",
    "#Put the correlation coefficient of each feature into the value list\n",
    "for v in cccc:\n",
    "    value.append(v)\n",
    "# value.sort()\n",
    "array = []\n",
    "\n",
    "#Put the name of each feature into the array list\n",
    "for column in conduct:\n",
    "    array.append(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_features = {}\n",
    "cor = []\n",
    "features_name = []\n",
    "labels = ['CX','CY','CZ']\n",
    "#Build a dictionary dict_features. The dictionary uses a key-value pair. The array is used as the key and the value is used as the value. It is added to the dictionary. At the same time, the value list is copied to the cor list.\n",
    "for i in range(len(array)):\n",
    "    if not np.isnan(value[i]) and not array[i] in labels:\n",
    "        dict_features[array[i]] = value[i]\n",
    "        features_name.append(array[i])\n",
    "        cor.append(value[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_v2n = {}\n",
    "\n",
    "#Remove features with a correlation coefficient of None from the dictionary, which is equivalent to cleaning the data\n",
    "for i in range(len(dict_features)):\n",
    "    co = str(cor[i])\n",
    "    if(dict_v2n.get(co) == None):\n",
    "        dict_v2n[co] = features_name[i]\n",
    "        \n",
    "#Sort the correlation coefficient from large to small, where sort() is sorted from small to large, reverse() reverses the process, and finally obtains the correlation coefficient sorted from large to small\n",
    "cor.sort()\n",
    "cor.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list = []\n",
    "#Add the names of the correlation coefficients to the features_list list in the order of the correlation coefficients from large to small\n",
    "for i in range(len(cor)):\n",
    "    co = str(cor[i])\n",
    "    if not dict_v2n[co] in features_list:\n",
    "        features_list.append(dict_v2n[co])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import itertools\n",
    "import _thread\n",
    "regr = make_pipeline(StandardScaler(),LinearSVR(random_state=0, tol=1e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sublist = []\n",
    "\n",
    "#forward select is responsible for calculating the error. The function is to add the feature at the specified index to the feature subset sublist, and use svm to train the feature subset, and return the error of the model\n",
    "def get_err(sublist,index):\n",
    "    sublist.append(features_list[index])\n",
    "    X = np.array(conduct[sublist])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,random_state = 32, shuffle = True)\n",
    "    svr_pre = regr.fit(X_train,y_train).predict(X_test)\n",
    "    return mean_absolute_percentage_error(y_test,svr_pre)\n",
    "\n",
    "#the entry of forward select\n",
    "def features_forward_select(sublist,delay_times):\n",
    "    global min_error\n",
    "    global final_sublist\n",
    "    left_index = 0\n",
    "    while left_index < len(features_list):     \n",
    "        mape_err = get_err(sublist,left_index)\n",
    "        #if the error of the current subset is less than the error of the minimum subset, then the current subset is the minimum subset, and the current error is the minimum error (the minimum subset refers to the feature subset with the minimum error)\n",
    "        if mape_err < min_error:\n",
    "            min_error = mape_err\n",
    "            final_sublist = sublist[:]\n",
    "            print(str(len(sublist)) + \",\" + str(min_error))\n",
    "            print(sublist)\n",
    "        #if the error of the current subset is not less than the error of the minimum subset, then remove the last feature of the current subset, which is equivalent to returning to the state when the last feature was added\n",
    "        #and start a depth search from this state until the maximum number of the maximum feature subset\n",
    "        else:\n",
    "            sublist.remove(sublist[len(sublist) - 1])\n",
    "            ok,index_return,list_return = delay_seek(sublist,delay_times,left_index)\n",
    "            if ok:\n",
    "                left_index = index_return\n",
    "                sublist = list_return\n",
    "            else:\n",
    "                break\n",
    "        left_index += 1\n",
    "    return sublist\n",
    "            \n",
    "# deep search function, delay_times represents the maximum number of searches, delay_times + init_index represents the maximum threshold for the search\n",
    "def delay_seek(sublist,delay_times,init_index):\n",
    "    global min_error\n",
    "    global final_sublist\n",
    "    if delay_times <= 0 or delay_times >= len(features_list):\n",
    "        print(\"Error, delay_times must be a positive number!\")\n",
    "        return False,None,None\n",
    "    for i in range(init_index,len(features_list)):\n",
    "        mape_err = get_err(sublist,i)\n",
    "        if mape_err < min_error:\n",
    "            min_error = mape_err\n",
    "            final_sublist = sublist[:]\n",
    "            print(str(len(sublist)) + \",\" + str(min_error))\n",
    "            print(sublist)\n",
    "            return True, i, sublist\n",
    "        else:\n",
    "            sublist.remove(sublist[len(sublist) - 1])\n",
    "    delay_times -= 1\n",
    "    if(delay_times > 0 and init_index < len(features_list)):\n",
    "        sublist.append(features_list[init_index])\n",
    "        ok,index_return,list_return = delay_seek(sublist,delay_times,init_index + 1)\n",
    "        if ok:\n",
    "            return ok,index_return,list_return\n",
    "        else:\n",
    "            sublist.remove(sublist[len(sublist) - 1])\n",
    "    return False,None,None\n",
    "\n",
    "\n",
    "# remove_err is responsible for calculating the error. The function is to remove the feature at the specified index from the feature subset sublist, and use svm to train the feature subset, and return the error of the model\n",
    "def remove_err(sublist,index):\n",
    "    sublist.remove(sublist[index])\n",
    "    X = np.array(conduct[sublist])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,random_state = 32, shuffle = True)\n",
    "    svr_pre = regr.fit(X_train,y_train).predict(X_test)\n",
    "    return r2_score(y_test,svr_pre),mean_absolute_percentage_error(y_test,svr_pre)\n",
    "\n",
    "# the entry of backward select\n",
    "def features_backward_select(sublist,delay_times):\n",
    "    global max_r2\n",
    "    global min_error\n",
    "    global final_sublist\n",
    "    right_index = len(sublist)\n",
    "    while right_index >= 0:\n",
    "        right_index -= 1\n",
    "        feature = sublist[right_index]\n",
    "        r2,mape_err = remove_err(sublist,right_index)\n",
    "        print(\"nowlist=\"+str(len(sublist)))\n",
    "\n",
    "        if mape_err < min_error and r2 > max_r2 :\n",
    "            min_error = mape_err\n",
    "            max_r2 = r2\n",
    "            final_sublist = sublist[:]\n",
    "            print(str(len(sublist)) + \",min_error=\" + str(min_error) + \",max_r2=\" + str(max_r2))\n",
    "        else:\n",
    "            sublist.insert(right_index,feature)\n",
    "            ok,index_return,list_return = delay_seek_backword(sublist,delay_times,right_index)\n",
    "            if ok:\n",
    "                right_index = index_return\n",
    "                sublist = list_return\n",
    "            else:\n",
    "                break\n",
    "    return sublist\n",
    "\n",
    "#deep search in backward select\n",
    "def delay_seek_backword(sublist,delay_times,init_index):\n",
    "    global max_r2\n",
    "    global min_error\n",
    "    global final_sublist\n",
    "    if delay_times <= 0 or delay_times >= len(sublist):\n",
    "        print(\"Error, delay_times must be a positive number!\")\n",
    "        return False,None,None\n",
    "    print(\"enter seek\")\n",
    "    for i in reversed(range(init_index)):\n",
    "        feature = sublist[i]\n",
    "        r2,mape_err = remove_err(sublist,i)\n",
    "        if mape_err < min_error and r2 > max_r2 :\n",
    "            min_error = mape_err\n",
    "            max_r2 = r2\n",
    "            final_sublist = sublist[:]\n",
    "            print(str(len(sublist)) + \",min_error=\" + str(min_error) + \",max_r2=\" + str(max_r2))\n",
    "            return True, i, sublist\n",
    "        else:\n",
    "            sublist.insert(i,feature)\n",
    "    delay_times -= 1\n",
    "    if(delay_times > 0):\n",
    "        feature = sublist[init_index]\n",
    "        sublist.remove(feature)\n",
    "        ok,index_return,list_return = delay_seek_backword(sublist,delay_times,init_index - 1)\n",
    "        if ok:\n",
    "            return ok,index_return,list_return\n",
    "        else:\n",
    "            sublist.insert(init_index,feature)\n",
    "    return False,None,None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the model error corresponding to feature subsets of different sizes, as a comparison, you can find a relatively stable and small error interval for feature selection\n",
    "init_list = []\n",
    "cur_error = 100\n",
    "for i in range(len(features_list)):\n",
    "    sublist = features_list[: i + 1]\n",
    "    X = np.array(conduct[sublist])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,random_state = 32, shuffle = True)\n",
    "    svr_pre = regr.fit(X_train,y_train).predict(X_test)\n",
    "    mape_err = mean_absolute_percentage_error(y_test,svr_pre)\n",
    "    if mape_err < cur_error:\n",
    "        init_list.append(features_list[i])\n",
    "    cur_error =  mape_err\n",
    "    r2 = r2_score(y_test,svr_pre)\n",
    "    print(str(mape_err) + \",len=\" + str(len(sublist)) + \"，r2=\" + str(r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(conduct[init_list])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,random_state = 32, shuffle = True)\n",
    "svr_pre = regr.fit(X_train,y_train).predict(X_test)\n",
    "mape_err = mean_absolute_percentage_error(y_test,svr_pre)\n",
    "mape_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(init_list)):\n",
    "    features_list.remove(init_list[i])\n",
    "print(len(features_list))\n",
    "print(len(init_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sublist = init_list\n",
    "left_index = 70\n",
    "delay_times = 6\n",
    "min_error = 0.06\n",
    "sublist = features_forward_select(sublist,delay_times)\n",
    "print()\n",
    "print(final_sublist)\n",
    "print(len(final_sublist))\n",
    "print(min_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delay_times = 15\n",
    "max_r2 = 0 \n",
    "sublist = features_backward_select(sublist,delay_times)\n",
    "print(final_sublist)\n",
    "print(len(final_sublist))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
